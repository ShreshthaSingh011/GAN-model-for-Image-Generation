# Convolutional Model to Generate Human and Anime Faces Using DCGAN

This repository contains an implementation of a Deep Convolutional Generative Adversarial Network (DCGAN) for generating human and anime faces. The model leverages deep learning techniques to generate realistic human faces as well as artistic anime faces from random noise input.

## Publication

The findings from this work have been published in the *International Conference on Communication and Intelligent Systems* by Springer Nature Singapore. For more details, please refer to the publication:

**Title**: A Convolutional Model to Generate Human and Anime Faces Using DCGAN  
**Published by**: Springer Nature Singapore  
**Conference**: International Conference on Communication and Intelligent Systems  
[Link to the publication (https://link.springer.com/chapter/10.1007/978-981-97-2053-8_18)]


## Table of Contents
- [Overview](#overview)
- [Getting Started](#getting-started)
- [Requirements](#requirements)
- [Model Architecture](#model-architecture)
- [Training Process](#training-process)
- [Dataset](#dataset)
- [Results](#results)
- [Usage](#usage)
- [License](#license)

## Overview

This project demonstrates how to use a DCGAN for generating faces using two different datasets:
- **Human Faces**: A dataset of real human faces (e.g., CelebA dataset).
- **Anime Faces**: A dataset of anime character faces (e.g., a dataset from the Anime Face Dataset or other related sources).

The model is trained to learn the distribution of these face images and generate new images based on random noise input.

## Getting Started

To get started with this project, you will need to clone this repository and set up the environment. Follow the instructions below to set up and run the model.

### Clone the Repository
```bash
git clone https://github.com/ShreshthaSingh011/GAN-model-for-Image-Generation
cd dcgan-human-anime-faces
```

### Install Dependencies
You can install the required dependencies using `pip`:
```bash
pip install -r requirements.txt
```

## Requirements

- Python 3.x
- TensorFlow 2.x or PyTorch (Depending on implementation)
- CUDA (Optional, for GPU acceleration)
- Other dependencies listed in `requirements.txt`

## Model Architecture

The model consists of two main components:
1. **Generator**: Takes a random noise vector as input and generates a fake face image.
2. **Discriminator**: Evaluates whether an image is real or generated by the generator.

Both networks are built using convolutional layers, and the architecture follows the DCGAN structure with LeakyReLU activations and batch normalization.

Key Points:
- The generator uses transposed convolutions (also known as deconvolutions) to upsample the latent vector into an image.
- The discriminator uses regular convolutions to classify the input image as real or fake.

## Training Process

1. **Data Preprocessing**: 
   - Images are resized and normalized to [-1, 1] for better training stability.
   
2. **Training Loop**:
   - The generator and discriminator are trained in alternation, where the discriminator is updated first to distinguish real and generated images, and then the generator is updated to improve its ability to fool the discriminator.
   
3. **Loss Functions**:
   - We use Binary Cross-Entropy loss for both the generator and the discriminator.

4. **Optimizer**:
   - Adam optimizer with a learning rate of 0.0002 and beta values (0.5, 0.999).

## Dataset

The model supports training on two datasets:
1. **Human Faces**: You can use datasets like the [CelebA dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) which contains labeled human face images.
2. **Anime Faces**: You can use datasets like the [Anime Face Dataset](https://www.kaggle.com/datasets/sbhatti/real-or-not) or any other related dataset of anime characters.

Make sure to download the datasets and place them in the appropriate directory (`./data`).

## Results

After training the model, the generator should be able to produce high-quality human or anime faces. The images are saved periodically during training for evaluation.
![image](https://github.com/user-attachments/assets/4fb3c265-b454-4fa9-8ead-657befd43ae8)

![image](https://github.com/user-attachments/assets/c54329ab-4d0f-40f1-9f63-24d82c1812cc)

![image](https://github.com/user-attachments/assets/60f0e429-b2d6-4308-9672-848d36bdc527)

![image](https://github.com/user-attachments/assets/31a5c1e7-487d-4418-ab67-f828f4f7a543)




### Example Outputs:
- **Human Faces**:
  ![Human Example](path_to_example_human_face_image.png)
  
- **Anime Faces**:
  ![Anime Example](path_to_example_anime_face_image.png)

## Usage

To train the model, run the following command:
```bash
python train.py --dataset human --epochs 50 --batch_size 64
```
Replace `human` with `anime` to train on the anime faces dataset.

To generate new images from a trained model:
```bash
python generate.py --model_path saved_model/generator.h5 --output_dir generated_images
```

This will output generated images to the specified directory.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
```
